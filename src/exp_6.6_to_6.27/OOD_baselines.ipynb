{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Out of Distribution Testing (for main result in paper)\n",
    "## FasterRisk in here is uncalibrated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mimic_pipeline as mmp\n",
    "import mimic_pipeline.utils as utils\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve\n",
    "\n",
    "plt.rcParams['figure.dpi'] = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eicu_df = pd.read_csv('data/eICU-union.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = eicu_df.drop(['uniquepid', 'patientunitstayid', 'hospital_expire_flag', 'apache_iv_prob', 'apache_iva_prob', 'oasis_prob', 'sapsii_prob'], axis=1), eicu_df['hospital_expire_flag']\n",
    "apacheiv, apacheiva, oasis, sapsii = eicu_df['apache_iv_prob'], eicu_df['apache_iva_prob'], eicu_df['oasis_prob'], eicu_df['sapsii_prob']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_fasterrisk(load_path: str, X_test, y_test, baselines: dict) -> dict:\n",
    "    fasterrisk_name = load_path.split('/')[-1]\n",
    "    fasterrisk = joblib.load(load_path)\n",
    "    binarizer = joblib.load(f\"{load_path}-binarizer\")\n",
    "    stats = {}\n",
    "    \n",
    "    X_test, _ = binarizer.transform(X_test)\n",
    "    y_prob = fasterrisk.predict_proba(X_test.to_numpy())\n",
    "    \n",
    "    stats[f\"{fasterrisk_name}_y_prob\"] = y_prob\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "    stats[f\"{fasterrisk_name}_fpr\"], stats[f\"{fasterrisk_name}_tpr\"] = fpr, tpr\n",
    "    auroc = auc(fpr, tpr)\n",
    "    stats[f\"{fasterrisk_name}_auroc\"] = auroc\n",
    "    precision, recall, _ = precision_recall_curve(y_test, y_prob)\n",
    "    stats[f\"{fasterrisk_name}_precision\"], stats[f\"{fasterrisk_name}_recall\"] = precision, recall\n",
    "    auprc = auc(recall, precision)\n",
    "    stats[f\"{fasterrisk_name}_auprc\"] = auprc\n",
    "    prob_true, prob_pred, h_stat, pvalue = mmp.metric.get_calibration_curve(y_test, y_prob)\n",
    "    stats[f\"{fasterrisk_name}_prob_true\"], stats[f\"{fasterrisk_name}_prob_pred\"] = prob_true, prob_pred\n",
    "    stats[f\"{fasterrisk_name}_h_stat\"], stats[f\"{fasterrisk_name}_pvalue\"] = h_stat, pvalue\n",
    "    \n",
    "    for name, baseline_prob in baselines.items():\n",
    "        stats[f\"{name}_y_prob\"] = baseline_prob\n",
    "        fpr, tpr, _ = roc_curve(y_test, baseline_prob)\n",
    "        stats[f\"{name}_fpr\"], stats[f\"{name}_tpr\"] = fpr, tpr\n",
    "        stats[f'{name}_auroc'] = auc(fpr, tpr)\n",
    "        precision, recall, _ = precision_recall_curve(y_test, baseline_prob)\n",
    "        stats[f\"{name}_precision\"], stats[f\"{name}_recall\"] = precision, recall\n",
    "        stats[f'{name}_auprc'] = auc(recall, precision)\n",
    "        prob_true, prob_pred, h_stat, pvalue = mmp.metric.get_calibration_curve(y_test, baseline_prob)\n",
    "        stats[f\"{name}_prob_true\"], stats[f\"{name}_prob_pred\"] = prob_true, prob_pred\n",
    "        stats[f\"{name}_h_stat\"], stats[f\"{name}_pvalue\"] = h_stat, pvalue\n",
    "    \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_results(stats: dict, names: list, name_dict: dict, title: str):\n",
    "    sns.set_style(\"white\")\n",
    "    print(f\"{'-'*50} {title} {'-'*50}\")\n",
    "    for name in names:      # ROC\n",
    "        fpr, tpr = stats[f\"{name}_fpr\"], stats[f\"{name}_tpr\"]\n",
    "        if name.split('-')[0] == 'fasterrisk':\n",
    "            alpha, linewidth = 1, 1.5\n",
    "        else:\n",
    "            alpha, linewidth = 0.5, 1\n",
    "        ax = sns.lineplot(x=fpr, y=tpr, label=f\"{name_dict[name]}, {stats[f'{name}_auroc']:.3f}\", linewidth=linewidth, alpha=alpha)\n",
    "        ax.figure.set_size_inches(8, 8)\n",
    "    sns.lineplot(x=np.linspace(0,1), y=np.linspace(0,1), label='Random', linestyle='--', color='grey', linewidth=1)\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(title)\n",
    "    plt.legend(bbox_to_anchor=(1, 1), loc=\"upper left\")\n",
    "    plt.show()\n",
    "    \n",
    "    for name in names:      # PR\n",
    "        precision, recall = stats[f\"{name}_precision\"], stats[f\"{name}_recall\"]\n",
    "        if name.split('-')[0] == 'fasterrisk':\n",
    "            alpha, linewidth = 1, 1.5\n",
    "        else:\n",
    "            alpha, linewidth = 0.5, 1\n",
    "        ax = sns.lineplot(x=recall, y=precision, label=f\"{name_dict[name]}, {stats[f'{name}_auprc']:.3f}\", linewidth=linewidth, alpha=alpha)\n",
    "        ax.figure.set_size_inches(8, 8)\n",
    "        plt.legend(bbox_to_anchor=(1, 1), loc=\"upper left\")\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "    \n",
    "    for name in names:      # calibration\n",
    "        prob_true, prob_pred = stats[f\"{name}_prob_true\"], stats[f\"{name}_prob_pred\"]\n",
    "        if name.split('-')[0] == 'fasterrisk':\n",
    "            alpha, linewidth = 1, 1.5\n",
    "        else:\n",
    "            alpha, linewidth = 0.5, 1\n",
    "            \n",
    "        if stats[f'{name}_pvalue'] < 0.0001:\n",
    "            p_label = 'p < 0.0001'\n",
    "        else:\n",
    "            p_label = f'p = {stats[f\"{name}_pvalue\"]:.3f}'\n",
    "        ax = sns.lineplot(x=prob_pred, y=prob_true, label=f\"{name_dict[name]}, H = {stats[f'{name}_h_stat']:.3f}, {p_label} \", linewidth=linewidth, alpha=alpha, marker='s')\n",
    "        ax.figure.set_size_inches(8, 8)\n",
    "    sns.lineplot(x=np.linspace(0,1), y=np.linspace(0,1), label='Perfect', linestyle='--', color='grey', linewidth=1)\n",
    "    plt.xlabel('Predicted Probability')\n",
    "    plt.ylabel('True Probability')\n",
    "    plt.title(title)\n",
    "    plt.legend(bbox_to_anchor=(1, 1), loc=\"upper left\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sparsity in range(10, 50, 5):\n",
    "    stats = evaluate_fasterrisk(f'models/fasterrisk-{sparsity}', X_test, y_test, {'apacheiv': apacheiv, 'apacheiva': apacheiva, 'oasis': oasis, 'sapsii': sapsii})\n",
    "    visualize_results(\n",
    "        stats, \n",
    "        [f'fasterrisk-{sparsity}', 'apacheiv', 'apacheiva', 'oasis', 'sapsii'], \n",
    "        {f'fasterrisk-{sparsity}': f'FasterRisk-{sparsity}', 'apacheiv': 'APACHE IV', 'apacheiva': 'APACHE IVa', 'oasis': 'OASIS', 'sapsii': 'SAPS II'},\n",
    "        title=f\"FasterRisk with Group Sparsity of {sparsity}\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "474",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
